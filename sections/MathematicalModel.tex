\section{Mathematical Models}
\label{sec:mathmodel}
Based on data volume rates, we do need mathematical models to identify which are the source addresses that may be proponents of an attack. 
In letterature we found different approaches, \cite{detection_by_path_analaysis} categorise data into predictable and non predictable data, their mathematical models must be able judge the data by using a threshold. These possible models are the tools for self-similarity analysis such as correlation coefficient and distance matrix. They \cite{detection_by_path_analaysis} use \textit{Pearson's correlation coefficient}, which is defined as:

\begin{equation}
\label{eq:pearson_corr}
	\rho_{X, Y} = \frac{E[(X-\mu_{X})(Y-\mu_{Y})]}{\sigma_X\sigma_Y}
\end{equation}

The correlation is used to measure dependence between two quantities (variables) $X$ and $Y$ with expected values $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$. Both value of the standard deviations are finite and nonzero ($0 < \sigma_X< \infty$ and $0 < \sigma_Y < \infty$). One of the impressive properties of the correlation is symmetric measurement, in other words, whichever data comes first, we can still get the same result as measuring. In paper \cite{detection_by_path_analaysis} they propose several methodology algorithms, we report two of them
\begin{enumerate}
	\item Correlation between arrival rate and sequence number.
	Denoted $X$ as a sample set of arrival rate ($X = {\label{_k}}$, where $k=$ 0, 1, 2, ..., N) and $Y$ as a sample set of sequence number ($Y = {k}$  where $k=$ 0, 1, 2, ..., N). Calculate the correlation value $(\rho_{X, Y})$ from the two variables: $X$ and $Y$. The expected value is between $-1$ and $1$, $-1 > \rho_{X, Y} > 1 $.
	\item Correlation of self arrival rate. Denoted $X$ as a sample sequence of arrival rate ($X = {\lambda_{(2k)}}$, where $k=$ 0, 1, 2, ..., N) and $Y$ as a sequence number of time interval ($Y = {\lambda_{(2k + 1)}}$ where $k=$ 0, 1, 2, ..., N). Calculate the correlation value $(\rho_{X, Y})$ from the two variables: $X$ and $Y$. The expected value is between $-1$ and $1$, $-1 > \rho_{X, Y} > 1 $.
\end{enumerate}
In order to use the previous mentioned methodology, \cite{detection_by_path_analaysis} define two thresholds: upper $\tau_U$ and lower $\tau_L$. The value of the upper threshold must not exceed 1 and be less than the lower threshold, on the other hand, the value of the lower threshold must not be below 0 and greater than the upper threshold.
These thresholds will help to identify the dependency degree of dependency of the arrival rate data into two categories.
\begin{enumerate}
	\item \textit{Predictable attack rate}: the data will be classified as a predictable attack rate if the correlation value is closed to 0 or 1.
	\item \textit{Nonpredictable attack rate}: the data will be classified as a nonpredictable attack rate if the correlation value is not closed to 0 or 1.
\end{enumerate} 
Unfortunately, only one correlation result $\rho_{X, Y}$ cannot
determine whether arrival data is attacking or legitimate, it is
needed a series of correlation result to confirm the situation.
The attack detection must respond as quickly as possible after the attack reaches the victim by using these methods, doing an online analysis.

Since we categorise data, using big data framework in order to analyse log files offline, which may contains attacks, we can not use methodology proposed by \cite{detection_by_path_analaysis}. Using big data we elaborate the log files in order to extract features used for our analysis. For simplicity we explain each one by referring to a single entry which represents an exchange between a source IP and server.

\begin{itemize}
	\item \textit{n\_packets} \\ Represents the amount of packets.
	\item \textit{total\_volume} \\ It is the sum of all packets length.
	\item \textit{time\_difference} \\ It is the time difference between the first communication and the last one, we consider it as a time window.
	\item \textit{ratio\_vol\_td} \\ Represents the volume over seconds exchanged during the time window.
\end{itemize}

The mathematical model must be able to judge the data by using a threshold, in our case the maximal volume of traffic per second supported by the server under analysis represents this threshold. Other mathematical model are treated in\cite{detection_by_path_analaysis}. 

Using \textit{python} in order to manipulate the results returned by \textit{Pig} we decide to use \textbf{standard deviation} as graphical guideline model together with the mean and  \textbf{Quantile Range Outliers} method to identify suspicious source address mathematically.
The formula for the sample standard deviation is

\begin{equation}
\label{eq:standard_dev}
	s = \sqrt{\frac{1}{N-1}\sum_{i=1}^N(x_i - \bar{x})^2}
\end{equation}
where $x_1, x_2\ ...\ x_n$ are the samples values, $\bar{x}$ is the mean and $N$ is the number of samples in the dataset generated by \textit{Pig-latin} script.

The \textit{standard deviation} is a description of the data's spread, how widely it is distributed about the mean.  
A smaller standard deviation indicates that more of the data is clustered about the mean.  
A larger one indicates the data are more spread out.
Using this information, we started to make inferences about the data.  
For example, we could determine whether a particular point was significantly higher above data volume than all others,  whether it represented a statistical anomaly that was worth investigating, based on how many standard deviations away from the mean it was located.

The \textit{quantile range outliers} method of outlier detection uses the quantile distribution of the values in a column to locate the extreme values. Quantiles are useful for detecting outliers because there is no distributional assumption associated with them. Data are simply sorted from smallest to largest. For example, the 20th quantile is the value at which 20\% of values are smaller. Extreme values are found using a multiplier of the interquantile range, the distance between two specified quantiles.
In statistics and probability \textit{quantiles} are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. 

In our work we decide to identify outliers calculating the $25^{th}$ and the $75^{th}$ percentile then we calculate the interquantile rage by doing $Q_{75}-Q_{25}$. We chose $1.5$ as multiplicative $\alpha$ factor for the interquantile. 
We use this coefficient as \textit{cut off} for determine upper and lower bound for outliers data. 
The following listing report the code which implements this feature\footnote{In Lst. \ref{lst:qrom} we report only the method applied to data volume exchanged, we do the same for traffic volume (Kb/s)} .
 
 \begin{lstlisting}[numbers=left, columns=flexible, breaklines=true, frame=tb, caption={quantile range outliers method}, label={lst:qrom}]
    q25, q75 = percentile(dataframe['total_volume'], 25), percentile(dataframe['total_volume'], 75)
    inter_quartile_range = q75 - q25
    # calculate the outlier cutoff
    cut_off = inter_quartile_range * 1.5
    lower, upper = q25 - cut_off, q75 + cut_off
    # identify outliers
    data_outliers = [x for x in dataframe['total_volume'] if x < lower or x > upper]
\end{lstlisting}

We decide to output two plots and one file, \textit{data analysis} which figure out the amount of data, in terms of megabyte, exchanged between a source and the server. \textit{Volume analysis} is like the previous one but represents the data volumes per second exchanged between a source and the server. In the \texttt{.txt} file we report the outliers by IP address and values of analysis we calculates, such as traffic volume and data exchanged.
\bigskip
